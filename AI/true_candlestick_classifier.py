import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
from typing import Dict, List, Tuple, Optional
import logging

class TrueCandlestickClassifier:
    """
    ‡∏™‡∏≠‡∏ô AI ‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
    ‡∏ï‡∏≤‡∏° Traditional Candlestick Analysis
    """
    
    def __init__(self, data_folder: str = "raw_ai_data_XAUUSD_c"):
        self.data_folder = data_folder
        self.data = {}
        self.scaler = None
        self.label_encoder = None
        self.model = None
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Load raw data
        self._load_raw_data()
    
    def _load_raw_data(self):
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Raw"""
        if not os.path.exists(self.data_folder):
            self.logger.error(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {self.data_folder}")
            return
        
        self.logger.info("üìÇ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Raw Candlestick...")
        
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°
        timeframes = ['M1', 'M5', 'M30', 'H1', 'H4', 'D1']
        
        all_data = []
        successful_loads = 0
        
        for tf in timeframes:
            file_path = f"{self.data_folder}/XAUUSD.c_{tf}_raw.csv"
            
            if os.path.exists(file_path):
                try:
                    df = pd.read_csv(file_path, index_col=0, parse_dates=True)
                    df['Timeframe'] = tf  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° timeframe weight (‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤)
                    tf_weights = {
                        'M1': 1.0,
                        'M5': 1.2, 
                        'M30': 1.5,
                        'H1': 2.0,
                        'H4': 3.0,
                        'D1': 4.0
                    }
                    df['TF_weight'] = tf_weights[tf]
                    
                    all_data.append(df)
                    successful_loads += 1
                    self.logger.info(f"‚úÖ {tf}: {len(df):,} ‡πÅ‡∏ó‡πà‡∏á")
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è  {tf}: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ - {str(e)}")
            else:
                self.logger.warning(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file_path}")
        
        if all_data:
            # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°
            self.data = pd.concat(all_data, ignore_index=False)
            self.logger.info(f"üìä ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(self.data):,} ‡πÅ‡∏ó‡πà‡∏á ‡∏à‡∏≤‡∏Å {successful_loads} ‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°
            tf_distribution = self.data['Timeframe'].value_counts()
            self.logger.info("üìã ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°:")
            for tf, count in tf_distribution.items():
                percentage = (count / len(self.data)) * 100
                self.logger.info(f"   {tf}: {count:,} ‡πÅ‡∏ó‡πà‡∏á ({percentage:.1f}%)")
        else:
            self.logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡∏¢")
    
    def create_candlestick_anatomy_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏¢‡∏ß‡∏¥‡∏†‡∏≤‡∏Ñ‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô
        """
        df_features = df.copy()
        
        # === BASIC ANATOMY ===
        # Body (‡∏ï‡∏±‡∏ß‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô)
        df_features['Body_size'] = abs(df['Close'] - df['Open'])
        df_features['Body_direction'] = np.where(df['Close'] > df['Open'], 1, -1)  # 1=Bullish, -1=Bearish
        
        # Shadows (‡πÄ‡∏á‡∏≤)
        df_features['Upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])
        df_features['Lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']
        
        # Total range
        df_features['Total_range'] = df['High'] - df['Low']
        
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå
        df_features['Total_range'] = df_features['Total_range'].replace(0, 1e-8)
        
        # === RATIOS (‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) ===
        # Body ratio = ‡∏ï‡∏±‡∏ß‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô / ‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        df_features['Body_ratio'] = df_features['Body_size'] / df_features['Total_range']
        
        # Shadow ratios
        df_features['Upper_shadow_ratio'] = df_features['Upper_shadow'] / df_features['Total_range']
        df_features['Lower_shadow_ratio'] = df_features['Lower_shadow'] / df_features['Total_range']
        
        # === POSITIONS (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤) ===
        # ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á Open ‡πÅ‡∏•‡∏∞ Close ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤ (0-1)
        df_features['Open_position'] = (df['Open'] - df['Low']) / df_features['Total_range']
        df_features['Close_position'] = (df['Close'] - df['Low']) / df_features['Total_range']
        
        # === SYMMETRY (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•) ===
        # Shadow symmetry
        total_shadow = df_features['Upper_shadow'] + df_features['Lower_shadow']
        total_shadow = total_shadow.replace(0, 1e-8)
        df_features['Shadow_symmetry'] = abs(df_features['Upper_shadow'] - df_features['Lower_shadow']) / total_shadow
        
        # === SIZE CATEGORIES ===
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô relative ‡∏Å‡∏±‡∏ö recent ATR
        atr_period = 14
        df_features['ATR'] = df_features['Total_range'].rolling(atr_period).mean()
        df_features['Size_vs_ATR'] = df_features['Total_range'] / df_features['ATR']
        df_features['Body_vs_ATR'] = df_features['Body_size'] / df_features['ATR']
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        df_features = df_features.replace([np.inf, -np.inf], np.nan)
        df_features = df_features.fillna(method='ffill').fillna(0)
        
        return df_features
    
    def classify_candlestick_patterns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡∏ï‡∏≤‡∏° Traditional Candlestick Analysis
        ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ Ground Truth ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏≠‡∏ô AI
        """
        df_classified = df.copy()
        
        # === TRADITIONAL CANDLESTICK PATTERNS ===
        
        # 1. DOJI (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏á‡πÄ‡∏•, ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à)
        doji_threshold = 0.1
        is_doji = df['Body_ratio'] <= doji_threshold
        
        # 2. MARUBOZU (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ô‡πà‡∏ß‡πÅ‡∏ô‡πà, momentum ‡πÅ‡∏£‡∏á)
        marubozu_body_threshold = 0.8
        marubozu_shadow_threshold = 0.1
        is_marubozu = (
            (df['Body_ratio'] >= marubozu_body_threshold) &
            (df['Upper_shadow_ratio'] <= marubozu_shadow_threshold) &
            (df['Lower_shadow_ratio'] <= marubozu_shadow_threshold)
        )
        
        # 3. HAMMER (‡∏Å‡∏≤‡∏£‡∏Å‡∏•‡∏±‡∏ö‡∏ï‡∏±‡∏ß bullish ‡∏ó‡∏µ‡πà support)
        hammer_body_threshold = 0.3
        hammer_lower_threshold = 0.6
        hammer_upper_threshold = 0.1
        is_hammer = (
            (df['Body_ratio'] <= hammer_body_threshold) &
            (df['Lower_shadow_ratio'] >= hammer_lower_threshold) &
            (df['Upper_shadow_ratio'] <= hammer_upper_threshold)
        )
        
        # 4. SHOOTING STAR (‡∏Å‡∏≤‡∏£‡∏Å‡∏•‡∏±‡∏ö‡∏ï‡∏±‡∏ß bearish ‡∏ó‡∏µ‡πà resistance)
        star_body_threshold = 0.3
        star_upper_threshold = 0.6
        star_lower_threshold = 0.1
        is_shooting_star = (
            (df['Body_ratio'] <= star_body_threshold) &
            (df['Upper_shadow_ratio'] >= star_upper_threshold) &
            (df['Lower_shadow_ratio'] <= star_lower_threshold)
        )
        
        # 5. SPINNING TOP (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô, consolidation)
        spinning_body_threshold = 0.3
        spinning_shadow_threshold = 0.3
        is_spinning_top = (
            (df['Body_ratio'] <= spinning_body_threshold) &
            (df['Upper_shadow_ratio'] >= spinning_shadow_threshold) &
            (df['Lower_shadow_ratio'] >= spinning_shadow_threshold) &
            ~is_doji  # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà doji
        )
        
        # 6. LONG BODY (‡πÅ‡∏£‡∏á‡∏ã‡∏∑‡πâ‡∏≠/‡∏Ç‡∏≤‡∏¢‡πÅ‡∏£‡∏á)
        long_body_threshold = 0.6
        is_long_bullish = (
            (df['Body_ratio'] >= long_body_threshold) &
            (df['Body_direction'] == 1) &
            ~is_marubozu
        )
        
        is_long_bearish = (
            (df['Body_ratio'] >= long_body_threshold) &
            (df['Body_direction'] == -1) &
            ~is_marubozu
        )
        
        # 7. SMALL BODY (momentum ‡∏≠‡πà‡∏≠‡∏ô)
        small_body_threshold = 0.4
        is_small_body = (
            (df['Body_ratio'] <= small_body_threshold) &
            ~is_doji & ~is_hammer & ~is_shooting_star & ~is_spinning_top
        )
        
        # === ASSIGN PATTERNS ===
        conditions = [
            is_doji,
            is_marubozu & (df['Body_direction'] == 1),   # Bullish Marubozu
            is_marubozu & (df['Body_direction'] == -1),  # Bearish Marubozu
            is_hammer,
            is_shooting_star,
            is_spinning_top,
            is_long_bullish,
            is_long_bearish,
            is_small_body & (df['Body_direction'] == 1), # Small Bullish
            is_small_body & (df['Body_direction'] == -1), # Small Bearish
        ]
        
        choices = [
            'DOJI',
            'MARUBOZU_BULL',
            'MARUBOZU_BEAR', 
            'HAMMER',
            'SHOOTING_STAR',
            'SPINNING_TOP',
            'LONG_BULL',
            'LONG_BEAR',
            'SMALL_BULL',
            'SMALL_BEAR'
        ]
        
        df_classified['Candlestick_pattern'] = np.select(conditions, choices, default='NORMAL')
        
        # === MARKET PSYCHOLOGY ===
        # ‡πÅ‡∏õ‡∏•‡∏á pattern ‡πÄ‡∏õ‡πá‡∏ô market psychology
        psychology_map = {
            'DOJI': 'INDECISION',
            'MARUBOZU_BULL': 'STRONG_BULLISH',
            'MARUBOZU_BEAR': 'STRONG_BEARISH',
            'HAMMER': 'BULLISH_REVERSAL',
            'SHOOTING_STAR': 'BEARISH_REVERSAL', 
            'SPINNING_TOP': 'UNCERTAINTY',
            'LONG_BULL': 'BULLISH',
            'LONG_BEAR': 'BEARISH',
            'SMALL_BULL': 'WEAK_BULLISH',
            'SMALL_BEAR': 'WEAK_BEARISH',
            'NORMAL': 'NEUTRAL'
        }
        
        df_classified['Market_psychology'] = df_classified['Candlestick_pattern'].map(psychology_map)
        
        return df_classified
    
    def prepare_classification_data(self) -> Dict:
        """
        ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô Candlestick Classifier
        """
        if self.data.empty:
            raise ValueError("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô")
        
        self.logger.info("üîß ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Candlestick Classification...")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
        df_features = self.create_candlestick_anatomy_features(self.data)
        
        # ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô (Ground Truth)
        df_classified = self.classify_candlestick_patterns(df_features)
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI
        feature_columns = [
            'Body_ratio', 'Upper_shadow_ratio', 'Lower_shadow_ratio',
            'Open_position', 'Close_position', 'Shadow_symmetry',
            'Size_vs_ATR', 'Body_vs_ATR', 'Body_direction',
            'Hour', 'Day_of_week'  # Time context
        ]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°
        if 'Timeframe' in df_classified.columns:
            # Encode timeframe ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
            tf_mapping = {'M1': 1, 'M5': 5, 'M30': 30, 'H1': 60, 'H4': 240, 'D1': 1440}
            df_classified['TF_minutes'] = df_classified['Timeframe'].map(tf_mapping)
            feature_columns.append('TF_minutes')
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° timeframe weight
            if 'TF_weight' in df_classified.columns:
                feature_columns.append('TF_weight')
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ
        available_features = [col for col in feature_columns if col in df_classified.columns]
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° X ‡πÅ‡∏•‡∏∞ y
        X = df_classified[available_features].copy()
        y = df_classified['Candlestick_pattern'].copy()
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        valid_indices = X.notna().all(axis=1) & y.notna()
        X = X[valid_indices]
        y = y[valid_indices]
        
        # Time-based split
        split_date = '2024-01-01'
        train_mask = X.index < split_date
        test_mask = X.index >= split_date
        
        X_train = X[train_mask]
        y_train = y[train_mask]
        X_test = X[test_mask]
        y_test = y[test_mask]
        
        self.logger.info(f"üìä Training: {len(X_train):,} ‡πÅ‡∏ó‡πà‡∏á")
        self.logger.info(f"üìä Testing: {len(X_test):,} ‡πÅ‡∏ó‡πà‡∏á")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°
        if 'Timeframe' in df_classified.columns:
            self.logger.info("üìã ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡∏≤‡∏°‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°:")
            train_tf_dist = df_classified.loc[X_train.index, 'Timeframe'].value_counts()
            for tf, count in train_tf_dist.items():
                percentage = (count / len(X_train)) * 100
                self.logger.info(f"   {tf}: {count:,} ‡πÅ‡∏ó‡πà‡∏á ({percentage:.1f}%)")
        
        self.logger.info(f"üéØ Pattern distribution:")
        pattern_counts = y_train.value_counts()
        for pattern, count in pattern_counts.items():
            percentage = (count / len(y_train)) * 100
            self.logger.info(f"   {pattern}: {count:,} ({percentage:.1f}%)")
        
        return {
            'X_train': X_train,
            'y_train': y_train,
            'X_test': X_test,
            'y_test': y_test,
            'feature_names': available_features,
            'pattern_counts': pattern_counts,
            'timeframe_distribution': train_tf_dist if 'Timeframe' in df_classified.columns else None
        }
    
    def create_classifier_model(self, input_shape: int, num_classes: int) -> Model:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Candlestick Pattern Classifier
        """
        self.logger.info("üß† ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Candlestick Pattern Classifier")
        
        inputs = Input(shape=(input_shape,), name='candlestick_anatomy')
        
        # Feature extraction
        x = Dense(128, activation='relu', name='anatomy_analysis')(inputs)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        
        x = Dense(64, activation='relu', name='pattern_recognition')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.2)(x)
        
        x = Dense(32, activation='relu', name='psychology_understanding')(x)
        x = Dropout(0.1)(x)
        
        # Output layer
        outputs = Dense(num_classes, activation='softmax', name='pattern_classification')(x)
        
        model = Model(inputs=inputs, outputs=outputs, name='CandlestickPatternClassifier')
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def train_classifier(self) -> Dict:
        """
        ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Candlestick Pattern Classifier
        """
        self.logger.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô Candlestick Pattern Classifier")
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        data_splits = self.prepare_classification_data()
        
        X_train = data_splits['X_train']
        y_train = data_splits['y_train']
        X_test = data_splits['X_test']
        y_test = data_splits['y_test']
        
        # Encode labels
        self.label_encoder = LabelEncoder()
        y_train_encoded = self.label_encoder.fit_transform(y_train)
        y_test_encoded = self.label_encoder.transform(y_test)
        
        # Scale features
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
        self.model = self.create_classifier_model(X_train_scaled.shape[1], len(self.label_encoder.classes_))
        
        # Callbacks
        callbacks = [
            EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, mode='max'),
            ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=8, min_lr=1e-6, mode='max')
        ]
        
        self.logger.info("üî• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô...")
        
        # ‡πÄ‡∏ó‡∏£‡∏ô
        history = self.model.fit(
            X_train_scaled, y_train_encoded,
            epochs=100,
            batch_size=128,
            validation_data=(X_test_scaled, y_test_encoded),
            callbacks=callbacks,
            verbose=1
        )
        
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
        test_loss, test_accuracy = self.model.evaluate(X_test_scaled, y_test_encoded, verbose=0)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏î‡∏π confusion matrix
        y_pred = self.model.predict(X_test_scaled)
        y_pred_classes = np.argmax(y_pred, axis=1)
        
        # Classification report
        class_report = classification_report(
            y_test_encoded, y_pred_classes,
            target_names=self.label_encoder.classes_,
            output_dict=True
        )
        
        confusion_mat = confusion_matrix(y_test_encoded, y_pred_classes)
        
        self.logger.info(f"‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        self.logger.info(f"üéØ Test Accuracy: {test_accuracy:.4f}")
        self.logger.info(f"üìä Model ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å {len(self.label_encoder.classes_)} ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÅ‡∏ï‡πà‡∏•‡∏∞ pattern
        self.logger.info("üìã ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÅ‡∏ï‡πà‡∏•‡∏∞ Pattern:")
        for pattern in self.label_encoder.classes_:
            if pattern in class_report:
                precision = class_report[pattern]['precision']
                recall = class_report[pattern]['recall']
                f1 = class_report[pattern]['f1-score']
                support = class_report[pattern]['support']
                self.logger.info(f"   {pattern}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}, N={support}")
        
        return {
            'model': self.model,
            'history': history,
            'test_accuracy': test_accuracy,
            'test_loss': test_loss,
            'class_report': class_report,
            'confusion_matrix': confusion_mat,
            'pattern_counts': data_splits['pattern_counts'],
            'feature_names': data_splits['feature_names']
        }
    
    def visualize_results(self, results: Dict):
        """
        ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Candlestick Classifier
        """
        plt.figure(figsize=(20, 12))
        
        # 1. Training History
        plt.subplot(3, 4, 1)
        history = results['history']
        plt.plot(history.history['accuracy'], label='Training', linewidth=2)
        plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)
        plt.title('Model Accuracy', fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 2)
        plt.plot(history.history['loss'], label='Training', linewidth=2)
        plt.plot(history.history['val_loss'], label='Validation', linewidth=2)
        plt.title('Model Loss', fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. Confusion Matrix
        plt.subplot(3, 4, 3)
        conf_matrix = results['confusion_matrix']
        pattern_names = self.label_encoder.classes_
        
        sns.heatmap(conf_matrix, annot=True, fmt='d',
                   xticklabels=pattern_names, yticklabels=pattern_names,
                   cmap='Blues')
        plt.title('Confusion Matrix', fontweight='bold')
        plt.xlabel('Predicted Pattern')
        plt.ylabel('True Pattern')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        
        # 3. Pattern Distribution
        plt.subplot(3, 4, 4)
        pattern_counts = results['pattern_counts']
        plt.bar(pattern_counts.index, pattern_counts.values, alpha=0.8)
        plt.title('Pattern Distribution in Training', fontweight='bold')
        plt.xlabel('Candlestick Pattern')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        
        # 4. Per-Pattern Performance
        plt.subplot(3, 4, 5)
        class_report = results['class_report']
        patterns = [p for p in pattern_names if p in class_report]
        precisions = [class_report[p]['precision'] for p in patterns]
        recalls = [class_report[p]['recall'] for p in patterns]
        f1_scores = [class_report[p]['f1-score'] for p in patterns]
        
        x = np.arange(len(patterns))
        width = 0.25
        
        plt.bar(x - width, precisions, width, label='Precision', alpha=0.8)
        plt.bar(x, recalls, width, label='Recall', alpha=0.8)  
        plt.bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8)
        
        plt.title('Per-Pattern Performance', fontweight='bold')
        plt.xlabel('Pattern')
        plt.ylabel('Score')
        plt.xticks(x, patterns, rotation=45)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 5. Sample Predictions
        plt.subplot(3, 4, 6)
        plt.axis('off')
        
        summary_text = f"""
üïØÔ∏è CANDLESTICK AI CLASSIFIER

‚úÖ MISSION: ‡∏™‡∏≠‡∏ô AI ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô

üìä PERFORMANCE:
‚Ä¢ Accuracy: {results['test_accuracy']:.3f}
‚Ä¢ Patterns: {len(pattern_names)}
‚Ä¢ Features: {len(results['feature_names'])}

üéØ PATTERNS LEARNED:
‚Ä¢ DOJI (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏á‡πÄ‡∏•)
‚Ä¢ HAMMER (Bullish Reversal)  
‚Ä¢ SHOOTING STAR (Bearish Reversal)
‚Ä¢ MARUBOZU (Strong Momentum)
‚Ä¢ SPINNING TOP (Uncertainty)
‚Ä¢ LONG BODY (Strong Direction)

üß† AI ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à:
- ‡∏Å‡∏≤‡∏¢‡∏ß‡∏¥‡∏†‡∏≤‡∏Ñ‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô
- Market Psychology
- Traditional Patterns

‚úÖ READY FOR NEXT PHASE:
- Pattern Sequences
- Multi-Timeframe Context
- Trading Decisions
        """
        
        plt.text(0.05, 0.95, summary_text, transform=plt.gca().transAxes,
                fontsize=10, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
        
        plt.tight_layout()
        plt.show()
    
    def predict_candlestick(self, ohlc_data: Dict, timeframe: str = 'H1') -> Dict:
        """
        ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô
        
        Args:
            ohlc_data: {'Open': float, 'High': float, 'Low': float, 'Close': float}
            timeframe: ‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡∏ô‡∏µ‡πâ (default: 'H1')
        
        Returns:
            {'pattern': str, 'psychology': str, 'confidence': float}
        """
        if self.model is None or self.scaler is None:
            raise ValueError("‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡∏£‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡∏Å‡πà‡∏≠‡∏ô")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å OHLC
        o, h, l, c = ohlc_data['Open'], ohlc_data['High'], ohlc_data['Low'], ohlc_data['Close']
        
        body_size = abs(c - o)
        body_direction = 1 if c > o else -1
        upper_shadow = h - max(o, c)
        lower_shadow = min(o, c) - l
        total_range = h - l
        
        if total_range == 0:
            total_range = 1e-8
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        features = {
            'Body_ratio': body_size / total_range,
            'Upper_shadow_ratio': upper_shadow / total_range,
            'Lower_shadow_ratio': lower_shadow / total_range,
            'Open_position': (o - l) / total_range,
            'Close_position': (c - l) / total_range,
            'Shadow_symmetry': abs(upper_shadow - lower_shadow) / (upper_shadow + lower_shadow + 1e-8),
            'Size_vs_ATR': 1.0,  # ‡πÑ‡∏°‡πà‡∏°‡∏µ ATR context
            'Body_vs_ATR': 1.0,
            'Body_direction': body_direction,
            'Hour': 12,  # Default
            'Day_of_week': 1  # Default
        }
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏° (‡∏ñ‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        tf_mapping = {'M1': 1, 'M5': 5, 'M30': 30, 'H1': 60, 'H4': 240, 'D1': 1440}
        tf_weights = {'M1': 1.0, 'M5': 1.2, 'M30': 1.5, 'H1': 2.0, 'H4': 3.0, 'D1': 4.0}
        
        features['TF_minutes'] = tf_mapping.get(timeframe, 60)
        features['TF_weight'] = tf_weights.get(timeframe, 2.0)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á input array ‡∏ï‡∏≤‡∏° feature names ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        try:
            X = np.array([[features[col] for col in self.scaler.feature_names_in_]])
        except KeyError as e:
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ
            missing_feature = str(e).strip("'")
            self.logger.warning(f"‚ö†Ô∏è  ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå {missing_feature} ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á features array ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏£‡∏ö
            feature_values = []
            for col in self.scaler.feature_names_in_:
                if col in features:
                    feature_values.append(features[col])
                else:
                    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î
                    if 'TF_' in col:
                        feature_values.append(60)  # Default H1
                    elif 'Hour' in col:
                        feature_values.append(12)
                    elif 'Day' in col:
                        feature_values.append(1)
                    else:
                        feature_values.append(1.0)
            
            X = np.array([feature_values])
        
        X_scaled = self.scaler.transform(X)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        prediction = self.model.predict(X_scaled, verbose=0)
        predicted_class = np.argmax(prediction[0])
        confidence = prediction[0][predicted_class]
        
        pattern_name = self.label_encoder.inverse_transform([predicted_class])[0]
        
        # Map psychology
        psychology_map = {
            'DOJI': 'INDECISION',
            'MARUBOZU_BULL': 'STRONG_BULLISH',
            'MARUBOZU_BEAR': 'STRONG_BEARISH',
            'HAMMER': 'BULLISH_REVERSAL',
            'SHOOTING_STAR': 'BEARISH_REVERSAL',
            'SPINNING_TOP': 'UNCERTAINTY',
            'LONG_BULL': 'BULLISH',
            'LONG_BEAR': 'BEARISH',
            'SMALL_BULL': 'WEAK_BULLISH',
            'SMALL_BEAR': 'WEAK_BEARISH',
            'NORMAL': 'NEUTRAL'
        }
        
        psychology = psychology_map.get(pattern_name, 'NEUTRAL')
        
        return {
            'pattern': pattern_name,
            'psychology': psychology,
            'confidence': float(confidence),
            'timeframe': timeframe,
            'all_probabilities': {
                self.label_encoder.inverse_transform([i])[0]: float(prob)
                for i, prob in enumerate(prediction[0])
            }
        }
    
    def save_classifier(self, folder: str = "candlestick_classifier"):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
        if not os.path.exists(folder):
            os.makedirs(folder)
        
        if self.model:
            self.model.save(f"{folder}/candlestick_classifier.h5")
            joblib.dump(self.scaler, f"{folder}/scaler.pkl")
            joblib.dump(self.label_encoder, f"{folder}/label_encoder.pkl")
            self.logger.info(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {folder}/")

# === Usage Example ===
if __name__ == "__main__":
    print("üïØÔ∏è True Candlestick Pattern Classifier")
    print("‡∏™‡∏≠‡∏ô AI ‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó")
    print("=" * 60)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á classifier
    classifier = TrueCandlestickClassifier("raw_ai_data_XAUUSD_c")
    
    if classifier.data.empty:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô")
        exit()
    
    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(classifier.data):,} ‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô")
    
    try:
        # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        print("\nüöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô Candlestick Pattern Classifier...")
        results = classifier.train_classifier()
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        classifier.visualize_results(results)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        classifier.save_classifier()
        
        print(f"\n‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
        print(f"üß† AI ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô {len(classifier.label_encoder.classes_)} ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö!")
        print(f"üéØ Accuracy: {results['test_accuracy']:.3f}")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        print(f"\nüîÆ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏° (‡∏Ñ‡∏£‡∏ö 6 ‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°!)
        test_candles = [
            ({'Open': 2000, 'High': 2010, 'Low': 1990, 'Close': 2005}, 'M1', 'Normal Bullish M1'),
            ({'Open': 2000, 'High': 2003, 'Low': 1995, 'Close': 2001}, 'M5', 'Small Body M5'),
            ({'Open': 2000, 'High': 2005, 'Low': 1985, 'Close': 1999}, 'M30', 'Hammer-like M30'),
            ({'Open': 2000, 'High': 2010, 'Low': 1990, 'Close': 2005}, 'H1', 'Normal Bullish H1'),
            ({'Open': 2000, 'High': 2002, 'Low': 1980, 'Close': 1999}, 'H4', 'Hammer-like H4'),
            ({'Open': 2000, 'High': 2040, 'Low': 1995, 'Close': 2001}, 'D1', 'Shooting Star-like D1'),
        ]
        
        for candle_data, tf, description in test_candles:
            try:
                prediction = classifier.predict_candlestick(candle_data, tf)
                print(f"   {description}: {prediction['pattern']} | {prediction['psychology']} | {prediction['confidence']:.3f}")
            except Exception as e:
                print(f"   {description}: Error - {str(e)}")
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
        import traceback
        traceback.print_exc()