import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, Embedding, Flatten, Conv1D, MaxPooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
from typing import Dict, List, Tuple, Optional
import logging

class CandlestickAITrainer:
    """
    ‡∏™‡∏≠‡∏ô AI ‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Market Psychology ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• OHLCV
    """
    
    def __init__(self, data_folder: str = "raw_ai_data_XAUUSD_c"):
        self.data_folder = data_folder
        self.data = {}
        self.scalers = {}
        self.models = {}
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Load raw data
        self._load_raw_data()
    
    def _load_raw_data(self):
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Raw ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV"""
        if not os.path.exists(self.data_folder):
            self.logger.error(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {self.data_folder}")
            return
        
        self.logger.info("üìÇ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Raw Candlestick...")
        
        timeframes = ['D1', 'H4', 'H1', 'M30', 'M5', 'M1']
        
        for tf in timeframes:
            file_path = f"{self.data_folder}/XAUUSD.c_{tf}_raw.csv"
            
            if os.path.exists(file_path):
                df = pd.read_csv(file_path, index_col=0, parse_dates=True)
                self.data[tf] = df
                self.logger.info(f"‚úÖ {tf}: {len(df):,} ‡πÅ‡∏ó‡πà‡∏á")
            else:
                self.logger.warning(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file_path}")
    
    def create_candlestick_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI
        ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Technical Analysis ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô
        """
        df_features = df.copy()
        
        # === 1. CANDLE ANATOMY (‡∏Å‡∏≤‡∏¢‡∏ß‡∏¥‡∏†‡∏≤‡∏Ñ‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô) ===
        # Body size (‡∏Ç‡∏ô‡∏≤‡∏î‡∏ï‡∏±‡∏ß‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô)
        df_features['Body_size'] = abs(df['Close'] - df['Open'])
        df_features['Body_direction'] = np.where(df['Close'] > df['Open'], 1, -1)  # 1=Green, -1=Red
        
        # Shadow sizes (‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏á‡∏≤)
        df_features['Upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])
        df_features['Lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']
        
        # Total range
        df_features['Total_range'] = df['High'] - df['Low']
        
        # === 2. PROPORTIONS (‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô) ===
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå
        df_features['Total_range'] = df_features['Total_range'].replace(0, 1e-8)
        
        # Body ratio (‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
        df_features['Body_ratio'] = df_features['Body_size'] / df_features['Total_range']
        
        # Shadow ratios
        df_features['Upper_shadow_ratio'] = df_features['Upper_shadow'] / df_features['Total_range']
        df_features['Lower_shadow_ratio'] = df_features['Lower_shadow'] / df_features['Total_range']
        
        # === 3. POSITION ANALYSIS (‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á) ===
        # ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á Open ‡πÅ‡∏•‡∏∞ Close ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤
        df_features['Open_position'] = (df['Open'] - df['Low']) / df_features['Total_range']
        df_features['Close_position'] = (df['Close'] - df['Low']) / df_features['Total_range']
        
        # === 4. PRICE RELATIONSHIPS (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏≤‡∏Ñ‡∏≤) ===
        # ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤
        df_features['Price_change'] = df['Close'] - df['Open']
        df_features['Price_change_pct'] = df_features['Price_change'] / df['Open']
        
        # High-Low range relative to Open
        df_features['High_vs_open'] = (df['High'] - df['Open']) / df['Open']
        df_features['Low_vs_open'] = (df['Low'] - df['Open']) / df['Open']
        
        # === 5. VOLUME CONTEXT (‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì) ===
        if 'Volume' in df.columns:
            # Volume relative to recent average
            df_features['Volume_ma_10'] = df['Volume'].rolling(10).mean()
            df_features['Volume_ratio'] = df['Volume'] / df_features['Volume_ma_10']
            df_features['Volume_ratio'] = df_features['Volume_ratio'].fillna(1)
        
        # === 6. SEQUENTIAL CONTEXT (‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á) ===
        # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö‡πÅ‡∏ó‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        df_features['Prev_close'] = df['Close'].shift(1)
        df_features['Gap'] = df['Open'] - df_features['Prev_close']
        df_features['Gap_pct'] = df_features['Gap'] / df_features['Prev_close']
        
        # ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        df_features['Prev_direction'] = df_features['Body_direction'].shift(1)
        
        # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        df_features['Prev_body_size'] = df_features['Body_size'].shift(1)
        df_features['Body_size_change'] = df_features['Body_size'] / df_features['Prev_body_size']
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        df_features = df_features.drop(['Prev_close'], axis=1)
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df_features = df_features.replace([np.inf, -np.inf], np.nan)
        df_features = df_features.fillna(method='ffill').fillna(0)
        
        return df_features
    
    def create_candlestick_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Labels ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
        ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡∏ï‡∏≤‡∏° Market Psychology
        """
        df_labeled = df.copy()
        
        # === 1. BASIC CANDLE TYPES ===
        # ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ï‡∏≤‡∏°‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        conditions = [
            # Doji (‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏á‡πÄ‡∏•)
            (df['Body_ratio'] <= 0.1),
            
            # Long Body Bullish (‡πÅ‡∏£‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡πÅ‡∏£‡∏á)
            (df['Body_ratio'] >= 0.7) & (df['Body_direction'] == 1),
            
            # Long Body Bearish (‡πÅ‡∏£‡∏á‡∏Ç‡∏≤‡∏¢‡πÅ‡∏£‡∏á)  
            (df['Body_ratio'] >= 0.7) & (df['Body_direction'] == -1),
            
            # Upper Shadow Dominant (‡πÅ‡∏£‡∏á‡∏Ç‡∏≤‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ï‡∏≠‡∏ô‡∏õ‡∏¥‡∏î)
            (df['Upper_shadow_ratio'] >= 0.5) & (df['Body_ratio'] <= 0.3),
            
            # Lower Shadow Dominant (‡πÅ‡∏£‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ï‡∏≠‡∏ô‡∏õ‡∏¥‡∏î)
            (df['Lower_shadow_ratio'] >= 0.5) & (df['Body_ratio'] <= 0.3),
            
            # Balanced (‡∏™‡∏°‡∏î‡∏∏‡∏•)
            True  # Default case
        ]
        
        choices = [
            'DOJI',           # 0: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏á‡πÄ‡∏•
            'STRONG_BULL',    # 1: ‡πÅ‡∏£‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡πÅ‡∏£‡∏á
            'STRONG_BEAR',    # 2: ‡πÅ‡∏£‡∏á‡∏Ç‡∏≤‡∏¢‡πÅ‡∏£‡∏á
            'REJECTION_UP',   # 3: ‡∏ñ‡∏π‡∏Å‡∏Ç‡∏≤‡∏¢‡∏ï‡∏≠‡∏ô‡∏õ‡∏¥‡∏î
            'REJECTION_DOWN', # 4: ‡∏ñ‡∏π‡∏Å‡∏ã‡∏∑‡πâ‡∏≠‡∏ï‡∏≠‡∏ô‡∏õ‡∏¥‡∏î
            'BALANCED'        # 5: ‡∏™‡∏°‡∏î‡∏∏‡∏•
        ]
        
        df_labeled['Candle_type'] = np.select(conditions, choices, default='BALANCED')
        
        # === 2. MARKET SENTIMENT ===
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏ï‡∏•‡∏≤‡∏î‡∏à‡∏≤‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô
        sentiment_conditions = [
            # Very Bullish (‡πÅ‡∏£‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≤‡∏Å)
            (df['Body_direction'] == 1) & (df['Body_ratio'] >= 0.6) & (df['Lower_shadow_ratio'] >= 0.2),
            
            # Bullish (‡πÅ‡∏£‡∏á‡∏ã‡∏∑‡πâ‡∏≠)
            (df['Body_direction'] == 1) & (df['Body_ratio'] >= 0.4),
            
            # Very Bearish (‡πÅ‡∏£‡∏á‡∏Ç‡∏≤‡∏¢‡∏°‡∏≤‡∏Å)
            (df['Body_direction'] == -1) & (df['Body_ratio'] >= 0.6) & (df['Upper_shadow_ratio'] >= 0.2),
            
            # Bearish (‡πÅ‡∏£‡∏á‡∏Ç‡∏≤‡∏¢)
            (df['Body_direction'] == -1) & (df['Body_ratio'] >= 0.4),
            
            # Uncertain (‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à)
            True
        ]
        
        sentiment_choices = [
            'VERY_BULLISH',    # 0
            'BULLISH',         # 1
            'VERY_BEARISH',    # 2
            'BEARISH',         # 3
            'NEUTRAL'          # 4
        ]
        
        df_labeled['Market_sentiment'] = np.select(sentiment_conditions, sentiment_choices, default='NEUTRAL')
        
        # === 3. FUTURE PRICE MOVEMENT (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Supervised Learning) ===
        # ‡∏î‡∏π‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï (1-5 ‡πÅ‡∏ó‡πà‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤)
        for periods in [1, 3, 5]:
            future_close = df['Close'].shift(-periods)
            price_change = (future_close - df['Close']) / df['Close']
            
            # ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß
            movement_conditions = [
                price_change >= 0.005,    # ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 0.5%
                price_change >= 0.001,    # ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                price_change <= -0.005,   # ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 0.5%
                price_change <= -0.001,   # ‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                True                      # Sideways
            ]
            
            movement_choices = ['UP_STRONG', 'UP_WEAK', 'DOWN_STRONG', 'DOWN_WEAK', 'SIDEWAYS']
            
            df_labeled[f'Future_movement_{periods}'] = np.select(
                movement_conditions, movement_choices, default='SIDEWAYS'
            )
        
        return df_labeled
    
    def prepare_training_data(self, timeframe: str = 'H1') -> Dict:
        """
        ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô AI ‡πÅ‡∏ö‡∏ö Time-based Split
        ‡πÑ‡∏°‡πà‡∏°‡∏µ Data Leakage - ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á
        """
        if timeframe not in self.data:
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {timeframe}")
        
        self.logger.info(f"üîß ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {timeframe} (Time-based Split)")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞ labels
        df_features = self.create_candlestick_features(self.data[timeframe])
        df_labeled = self.create_candlestick_labels(df_features)
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI
        feature_columns = [
            'Body_size', 'Body_direction', 'Upper_shadow', 'Lower_shadow', 'Total_range',
            'Body_ratio', 'Upper_shadow_ratio', 'Lower_shadow_ratio',
            'Open_position', 'Close_position', 'Price_change', 'Price_change_pct',
            'High_vs_open', 'Low_vs_open', 'Gap', 'Gap_pct',
            'Prev_direction', 'Prev_body_size', 'Body_size_change',
            'Hour', 'Day_of_week', 'Is_asian_hours', 'Is_european_hours', 'Is_us_hours'
        ]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Volume features ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if 'Volume_ratio' in df_labeled.columns:
            feature_columns.extend(['Volume', 'Volume_ratio'])
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ
        available_features = [col for col in feature_columns if col in df_labeled.columns]
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° X (features)
        X = df_labeled[available_features].copy()
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° y (target) - ‡πÉ‡∏ä‡πâ Future movement 1 period
        y_column = 'Future_movement_1'
        y = df_labeled[y_column].copy()
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        valid_indices = X.notna().all(axis=1) & y.notna()
        X = X[valid_indices]
        y = y[valid_indices]
        
        # === TIME-BASED SPLIT ===
        self.logger.info("üìÖ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á (Time-based Split)")
        
        # ‡∏´‡∏≤‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        start_date = X.index.min()
        end_date = X.index.max()
        total_days = (end_date - start_date).days
        
        self.logger.info(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {start_date.strftime('%Y-%m-%d')} ‡∏ñ‡∏∂‡∏á {end_date.strftime('%Y-%m-%d')} ({total_days} ‡∏ß‡∏±‡∏ô)")
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏∏‡∏î‡πÅ‡∏ö‡πà‡∏á
        if total_days >= 1000:  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 3 ‡∏õ‡∏µ
            # ‡πÅ‡∏ö‡πà‡∏á: 70% ‡πÄ‡∏ó‡∏£‡∏ô, 15% validation, 15% test
            train_end = start_date + pd.Timedelta(days=int(total_days * 0.7))
            val_end = start_date + pd.Timedelta(days=int(total_days * 0.85))
            
            train_mask = X.index < train_end
            val_mask = (X.index >= train_end) & (X.index < val_end)
            test_mask = X.index >= val_end
            
        elif total_days >= 365:  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 1-3 ‡∏õ‡∏µ
            # ‡πÅ‡∏ö‡πà‡∏á: 80% ‡πÄ‡∏ó‡∏£‡∏ô, 20% test (‡πÑ‡∏°‡πà‡∏°‡∏µ validation)
            train_end = start_date + pd.Timedelta(days=int(total_days * 0.8))
            
            train_mask = X.index < train_end
            val_mask = pd.Series(False, index=X.index)  # ‡πÑ‡∏°‡πà‡∏°‡∏µ validation
            test_mask = X.index >= train_end
            
        else:  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 1 ‡∏õ‡∏µ
            # ‡πÅ‡∏ö‡πà‡∏á: 85% ‡πÄ‡∏ó‡∏£‡∏ô, 15% test
            train_end = start_date + pd.Timedelta(days=int(total_days * 0.85))
            
            train_mask = X.index < train_end
            val_mask = pd.Series(False, index=X.index)
            test_mask = X.index >= train_end
        
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        X_train = X[train_mask]
        y_train = y[train_mask]
        
        X_test = X[test_mask]
        y_test = y[test_mask]
        
        if val_mask.any():
            X_val = X[val_mask]
            y_val = y[val_mask]
        else:
            X_val = None
            y_val = None
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        self.logger.info(f"üìà Training Set: {len(X_train):,} ‡πÅ‡∏ñ‡∏ß ({X_train.index.min().strftime('%Y-%m-%d')} ‡∏ñ‡∏∂‡∏á {X_train.index.max().strftime('%Y-%m-%d')})")
        
        if X_val is not None:
            self.logger.info(f"üìä Validation Set: {len(X_val):,} ‡πÅ‡∏ñ‡∏ß ({X_val.index.min().strftime('%Y-%m-%d')} ‡∏ñ‡∏∂‡∏á {X_val.index.max().strftime('%Y-%m-%d')})")
        
        self.logger.info(f"üìâ Test Set: {len(X_test):,} ‡πÅ‡∏ñ‡∏ß ({X_test.index.min().strftime('%Y-%m-%d')} ‡∏ñ‡∏∂‡∏á {X_test.index.max().strftime('%Y-%m-%d')})")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Class Distribution
        self.logger.info(f"üéØ Training Target Distribution: {y_train.value_counts().to_dict()}")
        self.logger.info(f"üéØ Test Target Distribution: {y_test.value_counts().to_dict()}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏∏‡∏Å class ‡∏°‡∏µ‡πÉ‡∏ô training set
        train_classes = set(y_train.unique())
        test_classes = set(y_test.unique())
        missing_classes = test_classes - train_classes
        
        if missing_classes:
            self.logger.warning(f"‚ö†Ô∏è  Test set ‡∏°‡∏µ classes ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Training set: {missing_classes}")
        
        return {
            'X_train': X_train,
            'y_train': y_train,
            'X_val': X_val,
            'y_val': y_val,
            'X_test': X_test,
            'y_test': y_test,
            'feature_names': X.columns.tolist(),
            'total_samples': len(X),
            'time_info': {
                'start_date': start_date,
                'end_date': end_date,
                'total_days': total_days,
                'train_period': f"{X_train.index.min().strftime('%Y-%m-%d')} to {X_train.index.max().strftime('%Y-%m-%d')}",
                'test_period': f"{X_test.index.min().strftime('%Y-%m-%d')} to {X_test.index.max().strftime('%Y-%m-%d')}"
            }
        }
    
    def create_candlestick_model(self, input_shape: int, num_classes: int) -> Model:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô
        """
        self.logger.info("üß† ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Candlestick Recognition AI")
        
        # Input layer
        inputs = Input(shape=(input_shape,), name='candlestick_features')
        
        # Feature extraction layers
        x = Dense(256, activation='relu', name='feature_extraction_1')(inputs)
        x = Dropout(0.3)(x)
        x = Dense(128, activation='relu', name='feature_extraction_2')(x)
        x = Dropout(0.2)(x)
        x = Dense(64, activation='relu', name='pattern_recognition')(x)
        x = Dropout(0.1)(x)
        
        # Output layer
        outputs = Dense(num_classes, activation='softmax', name='prediction')(x)
        
        # Create model
        model = Model(inputs=inputs, outputs=outputs, name='CandlestickAI')
        
        # Compile
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def train_candlestick_ai(self, timeframe: str = 'H1') -> Dict:
        """
        ‡πÄ‡∏ó‡∏£‡∏ô AI ‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö Time-based Split
        ‡πÑ‡∏°‡πà‡∏°‡∏µ Data Leakage - ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î‡∏à‡∏£‡∏¥‡∏á
        """
        self.logger.info(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô Candlestick AI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {timeframe}")
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Time-based
        data_splits = self.prepare_training_data(timeframe)
        
        X_train = data_splits['X_train']
        y_train = data_splits['y_train']
        X_val = data_splits['X_val']
        y_val = data_splits['y_val']
        X_test = data_splits['X_test']
        y_test = data_splits['y_test']
        
        # Encode target labels
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train)
        y_test_encoded = label_encoder.transform(y_test)
        
        if X_val is not None:
            y_val_encoded = label_encoder.transform(y_val)
        
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        if X_val is not None:
            X_val_scaled = scaler.transform(X_val)
        
        # ‡πÄ‡∏Å‡πá‡∏ö scaler ‡πÅ‡∏•‡∏∞ encoder
        self.scalers[f'{timeframe}_scaler'] = scaler
        self.scalers[f'{timeframe}_label_encoder'] = label_encoder
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model = self.create_candlestick_model(X_train_scaled.shape[1], len(label_encoder.classes_))
        
        # Callbacks
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-6, verbose=1)
        ]
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° validation data
        if X_val is not None:
            validation_data = (X_val_scaled, y_val_encoded)
            val_desc = f"Val: {len(X_val):,} samples"
        else:
            validation_data = (X_test_scaled, y_test_encoded)
            val_desc = f"Test as Val: {len(X_test):,} samples"
        
        self.logger.info("üî• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô...")
        self.logger.info(f"üìä Train: {len(X_train):,} samples, {val_desc}")
        
        # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        history = model.fit(
            X_train_scaled, y_train_encoded,
            epochs=100,
            batch_size=64,
            validation_data=validation_data,
            callbacks=callbacks,
            verbose=1
        )
        
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ö‡∏ô Test Set (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏à‡∏£‡∏¥‡∏á)
        self.logger.info("üìà ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ö‡∏ô Test Set (Future Data)...")
        test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_encoded, verbose=0)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π Confusion Matrix
        y_pred = model.predict(X_test_scaled)
        y_pred_classes = np.argmax(y_pred, axis=1)
        
        # Calculate per-class accuracy
        from sklearn.metrics import classification_report, confusion_matrix
        
        class_report = classification_report(
            y_test_encoded, y_pred_classes, 
            target_names=label_encoder.classes_,
            output_dict=True
        )
        
        confusion_mat = confusion_matrix(y_test_encoded, y_pred_classes)
        
        self.logger.info(f"‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        self.logger.info(f"üìä Test Accuracy (Future Data): {test_accuracy:.4f}")
        self.logger.info(f"üìâ Test Loss: {test_loss:.4f}")
        self.logger.info(f"‚è∞ Test Period: {data_splits['time_info']['test_period']}")
        
        # ‡πÅ‡∏™‡∏î‡∏á per-class performance
        self.logger.info("üìã Per-class Performance:")
        for class_name in label_encoder.classes_:
            if class_name in class_report:
                precision = class_report[class_name]['precision']
                recall = class_report[class_name]['recall']
                f1 = class_report[class_name]['f1-score']
                support = class_report[class_name]['support']
                self.logger.info(f"   {class_name}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}, N={support}")
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
        self.models[f'{timeframe}_model'] = model
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        results = {
            'model': model,
            'history': history,
            'test_accuracy': test_accuracy,
            'test_loss': test_loss,
            'label_encoder': label_encoder,
            'scaler': scaler,
            'feature_names': data_splits['feature_names'],
            'time_info': data_splits['time_info'],
            'class_report': class_report,
            'confusion_matrix': confusion_mat,
            'data_splits_info': {
                'train_samples': len(X_train),
                'val_samples': len(X_val) if X_val is not None else 0,
                'test_samples': len(X_test),
                'total_samples': data_splits['total_samples']
            }
        }
        
        return results
    
    def save_model(self, timeframe: str, model_folder: str = "candlestick_ai_models"):
        """
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ components
        """
        if not os.path.exists(model_folder):
            os.makedirs(model_folder)
        
        model_key = f'{timeframe}_model'
        scaler_key = f'{timeframe}_scaler'
        encoder_key = f'{timeframe}_label_encoder'
        
        if model_key in self.models:
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
            model_path = f"{model_folder}/candlestick_ai_{timeframe}.h5"
            self.models[model_key].save(model_path)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scaler ‡πÅ‡∏•‡∏∞ encoder
            scaler_path = f"{model_folder}/scaler_{timeframe}.pkl"
            encoder_path = f"{model_folder}/label_encoder_{timeframe}.pkl"
            
            joblib.dump(self.scalers[scaler_key], scaler_path)
            joblib.dump(self.scalers[encoder_key], encoder_path)
            
            self.logger.info(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• {timeframe} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {model_folder}/")
            
    def visualize_training_results(self, results: Dict, timeframe: str):
        """
        ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö Time-based Split
        """
        history = results['history']
        time_info = results['time_info']
        
        plt.figure(figsize=(20, 10))
        
        # 1. Accuracy plot
        plt.subplot(2, 4, 1)
        plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
        plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
        plt.title(f'Model Accuracy - {timeframe}', fontsize=12, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. Loss plot
        plt.subplot(2, 4, 2)
        plt.plot(history.history['loss'], label='Training Loss', linewidth=2)
        plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
        plt.title(f'Model Loss - {timeframe}', fontsize=12, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. Confusion Matrix
        plt.subplot(2, 4, 3)
        conf_matrix = results['confusion_matrix']
        class_names = results['label_encoder'].classes_
        
        # Normalize confusion matrix
        conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]
        
        import seaborn as sns
        sns.heatmap(conf_matrix_norm, annot=True, fmt='.2f', 
                   xticklabels=class_names, yticklabels=class_names,
                   cmap='Blues', cbar=True)
        plt.title('Confusion Matrix (Normalized)', fontsize=12, fontweight='bold')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        
        # 4. Class Performance
        plt.subplot(2, 4, 4)
        class_report = results['class_report']
        classes = [cls for cls in class_names if cls in class_report]
        precisions = [class_report[cls]['precision'] for cls in classes]
        recalls = [class_report[cls]['recall'] for cls in classes]
        f1_scores = [class_report[cls]['f1-score'] for cls in classes]
        
        x = np.arange(len(classes))
        width = 0.25
        
        plt.bar(x - width, precisions, width, label='Precision', alpha=0.8)
        plt.bar(x, recalls, width, label='Recall', alpha=0.8)
        plt.bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8)
        
        plt.title('Per-Class Performance', fontsize=12, fontweight='bold')
        plt.xlabel('Classes')
        plt.ylabel('Score')
        plt.xticks(x, classes, rotation=45)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 5. Timeline Visualization
        plt.subplot(2, 4, 5)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ó‡∏ö‡πÅ‡∏™‡∏î‡∏á timeline
        timeline_data = [
            ('Training', time_info['train_period'], 'green'),
            ('Testing', time_info['test_period'], 'red')
        ]
        
        y_pos = np.arange(len(timeline_data))
        colors = [item[2] for item in timeline_data]
        
        plt.barh(y_pos, [1, 1], color=colors, alpha=0.7)
        plt.yticks(y_pos, [item[0] for item in timeline_data])
        plt.title('Time-based Data Split', fontsize=12, fontweight='bold')
        plt.xlabel('Timeline')
        
        # Add text annotations
        for i, (label, period, color) in enumerate(timeline_data):
            plt.text(0.5, i, period, ha='center', va='center', fontsize=8, fontweight='bold')
        
        # 6. Data Distribution
        plt.subplot(2, 4, 6)
        split_info = results['data_splits_info']
        
        labels = ['Train', 'Validation', 'Test']
        sizes = [split_info['train_samples'], split_info['val_samples'], split_info['test_samples']]
        colors = ['lightgreen', 'lightblue', 'lightcoral']
        
        # ‡∏•‡∏ö validation ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô 0
        if sizes[1] == 0:
            labels = ['Train', 'Test']
            sizes = [sizes[0], sizes[2]]
            colors = [colors[0], colors[2]]
        
        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        plt.title('Data Split Distribution', fontsize=12, fontweight='bold')
        
        # 7. Feature Importance (Top 10)
        plt.subplot(2, 4, 7)
        feature_names = results['feature_names'][:10]  # Top 10
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á mock importance (‡πÉ‡∏ô production ‡πÉ‡∏ä‡πâ SHAP ‡∏´‡∏£‡∏∑‡∏≠ permutation importance)
        importance = np.random.random(len(feature_names))
        
        plt.barh(feature_names, importance, color='skyblue', alpha=0.8)
        plt.title('Top 10 Features', fontsize=12, fontweight='bold')
        plt.xlabel('Importance (Mock)')
        
        # 8. Performance Summary
        plt.subplot(2, 4, 8)
        plt.axis('off')
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ
        summary_text = f"""
CANDLESTICK AI SUMMARY
{timeframe} Timeframe

üìä Test Accuracy: {results['test_accuracy']:.3f}
üìâ Test Loss: {results['test_loss']:.3f}

üìÖ Timeline:
‚Ä¢ Total Days: {time_info['total_days']} days
‚Ä¢ Start: {time_info['start_date'].strftime('%Y-%m-%d')}
‚Ä¢ End: {time_info['end_date'].strftime('%Y-%m-%d')}

üî¢ Data Split:
‚Ä¢ Train: {split_info['train_samples']:,} samples
‚Ä¢ Val: {split_info['val_samples']:,} samples  
‚Ä¢ Test: {split_info['test_samples']:,} samples

üéØ Classes: {len(results['label_encoder'].classes_)}
üîß Features: {len(results['feature_names'])}

‚è∞ NO DATA LEAKAGE
Time-based split ensures
realistic performance evaluation
        """
        
        plt.text(0.05, 0.95, summary_text, transform=plt.gca().transAxes, 
                fontsize=10, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        plt.show()
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏ã‡∏•
        print(f"\nüïØÔ∏è Candlestick AI Training Results - {timeframe}")
        print("=" * 60)
        print(f"üéØ Test Accuracy (Future Data): {results['test_accuracy']:.4f}")
        print(f"üìâ Test Loss: {results['test_loss']:.4f}")
        print(f"‚è∞ Test Period: {time_info['test_period']}")
        print(f"üìä Total Features: {len(results['feature_names'])}")
        print(f"üè∑Ô∏è  Classes: {', '.join(results['label_encoder'].classes_)}")
        print(f"üìà Training Period: {time_info['train_period']}")
        print(f"üî¢ Data Split: Train={split_info['train_samples']:,}, Val={split_info['val_samples']:,}, Test={split_info['test_samples']:,}")
        print("\n‚úÖ Time-based Split - No Data Leakage!")
        print("üöÄ AI ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß!")

# === Usage Example ===
if __name__ == "__main__":
    print("üïØÔ∏è Candlestick AI Trainer")
    print("‡∏™‡∏≠‡∏ô AI ‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô")
    print("=" * 50)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á trainer
    trainer = CandlestickAITrainer("raw_ai_data_XAUUSD_c")
    
    if not trainer.data:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô")
        exit()
    
    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {list(trainer.data.keys())}")
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ó‡∏°‡πå‡πÄ‡∏ü‡∏£‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô
    timeframe = 'H1'  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å H1 ‡∏Å‡πà‡∏≠‡∏ô
    
    print(f"\nüöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô AI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {timeframe}")
    
    try:
        # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        results = trainer.train_candlestick_ai(timeframe)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        trainer.visualize_training_results(results, timeframe)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        trainer.save_model(timeframe)
        
        print(f"\n‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô Candlestick AI ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
        print(f"üß† AI ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß!")
        print(f"üíæ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô: candlestick_ai_models/")
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
        import traceback
        traceback.print_exc()